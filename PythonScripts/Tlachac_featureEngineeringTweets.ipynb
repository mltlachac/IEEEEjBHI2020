{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code author: ML Tlachac\n",
    "#paper title: 'Screening for Depression with Retrospectively Harvested Private versus Public Text' \n",
    "#paper accessible at: https://ieeexplore-ieee-org.ezpxy-web-p-u01.wpi.edu/document/9049136\n",
    "#github: github.com/mltlachac/IEEEjBHI2020\n",
    "#https://ieeexplore-ieee-org.ezpxy-web-p-u01.wpi.edu/document/9049136\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import textblob as tb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moodable: (68328, 4)\n",
      "PHQ: (510, 3)\n",
      "unique PHQ: (501, 2)\n",
      "Moodable with PHQ: (66322, 7)\n",
      "EMU: (17058, 4)\n",
      "EMU last session: (17058, 6)\n",
      "PHQ: (126, 3)\n",
      "unique PHQ: (115, 2)\n",
      "EMU with PHQ: (17058, 7)\n"
     ]
    }
   ],
   "source": [
    "modality = \"Tweets\"\n",
    "ndays = 56\n",
    "\n",
    "#load Moodable data\n",
    "dft1 = pd.read_csv('dataTweetMoodableCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dft1 = dft1.dropna().reset_index()\n",
    "print(\"Moodable: \" + str(dft1.shape))\n",
    "\n",
    "#attach date data collected\n",
    "dfids1 = pd.read_csv(\"idsMoodableCommaQuoteWindows.csv\")\n",
    "dft1 = pd.merge(dfids1, dft1, on = \"id\")\n",
    "dft1 = dft1.reset_index()\n",
    "\n",
    "#attach PHQ-9 scores\n",
    "dft19 = pd.read_csv('phqsMoodableCommaQuoteWindows.csv')\n",
    "print(\"PHQ: \" + str(dft19.shape))\n",
    "\n",
    "scores = []\n",
    "for i in range(0, dft19.shape[0]):\n",
    "    content = dft19.content[i][1:-1].split(\",\")\n",
    "    summation = 0\n",
    "    for j in range(0, 9):\n",
    "        summation = summation + int(content[j][-2])\n",
    "    scores.append(summation)\n",
    "dft19[\"scores\"] = scores\n",
    "\n",
    "#limit to unique users, highest score is used\n",
    "idlist = []\n",
    "newScores = []\n",
    "for i in set(dft19[\"id\"]):\n",
    "    tempdf = dft19[dft19[\"id\"] == i].reset_index()\n",
    "    idlist.append(tempdf[\"id\"][0])\n",
    "    if tempdf.shape[0] > 1:\n",
    "        newScores.append(max(tempdf[\"scores\"]))\n",
    "    else:\n",
    "        newScores.append(tempdf.scores[0])\n",
    "unique19 = pd.DataFrame()\n",
    "unique19[\"id\"] = idlist\n",
    "unique19[\"scores\"] = newScores\n",
    "print(\"unique PHQ: \" + str(unique19.shape))\n",
    "\n",
    "dft1 = pd.merge(dft1, unique19, on = \"id\")\n",
    "print(\"Moodable with PHQ: \" + str(dft1.shape))\n",
    "\n",
    "#make ids represent dataset\n",
    "newids = []\n",
    "for m in range(0, dft1.shape[0]):\n",
    "    newids.append(\"m\" + str(dft1[\"id\"][m]))\n",
    "dft1[\"id\"] = newids\n",
    "\n",
    "#Load EMU data\n",
    "dft2 = pd.read_csv('dataTweetEMUCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dft2 = dft2.dropna().reset_index()\n",
    "print(\"EMU: \" + str(dft2.shape))\n",
    "\n",
    "#attach date data collected, limit EMU participants to last session with each phone\n",
    "dfids2start = pd.read_csv('idsEMUCommaQuoteWindows.csv', encoding = \"utf-8\")\n",
    "dfids2 = pd.DataFrame()\n",
    "dfids2[\"id\"] = dfids2start.sessionid\n",
    "dfids2[\"date\"] = dfids2start.date\n",
    "dfids2[\"paid\"] = dfids2start.paid\n",
    "dft2 = pd.merge(dfids2, dft2, on = \"id\")\n",
    "dft2 = dft2[dft2.paid == 2]\n",
    "df2t = dft2.reset_index()\n",
    "print(\"EMU last session: \" + str(dft2.shape))\n",
    "\n",
    "#attach PHQ-9 scores\n",
    "dft29 = pd.read_csv('phqsEMUCommaQuoteWindows.csv')\n",
    "print(\"PHQ: \" + str(dft29.shape))\n",
    "\n",
    "scores = []\n",
    "for i in range(0, dft29.shape[0]):\n",
    "    content = dft29.content[i][1:-1].split(\",\")\n",
    "    summation = 0\n",
    "    for j in range(0, 9):\n",
    "        summation = summation + int(content[j][-2])\n",
    "    scores.append(summation)\n",
    "dft29[\"scores\"] = scores\n",
    "\n",
    "#limit to unique users, highest score is used\n",
    "idlist = []\n",
    "newScores = []\n",
    "for i in set(dft29[\"id\"]):\n",
    "    tempdf = dft29[dft29[\"id\"] == i].reset_index()\n",
    "    idlist.append(tempdf[\"id\"][0])\n",
    "    if tempdf.shape[0] > 1:\n",
    "        newScores.append(max(tempdf[\"scores\"]))\n",
    "    else:\n",
    "        newScores.append(tempdf.scores[0])\n",
    "        \n",
    "unique29 = pd.DataFrame()\n",
    "unique29[\"id\"] = idlist\n",
    "unique29[\"scores\"] = newScores\n",
    "print(\"unique PHQ: \" + str(unique29.shape))\n",
    "\n",
    "dft2 = pd.merge(dft2, unique29, on = \"id\")\n",
    "print(\"EMU with PHQ: \" + str(dft2.shape))\n",
    "\n",
    "#make ids represent dataset\n",
    "newids = []\n",
    "for e in range(0, dft2.shape[0]):\n",
    "    newids.append(\"e\" + str(dft2[\"id\"][e]))\n",
    "dft2[\"id\"] = newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined: (83380, 5)\n",
      "(77309, 6)\n"
     ]
    }
   ],
   "source": [
    "dft1 = dft1.drop([\"level_0\"], axis = 1)\n",
    "dft2 = dft2.drop([\"paid\"], axis = 1)\n",
    "\n",
    "#Combine datasets\n",
    "dft = dft1.append(dft2)\n",
    "dft = dft.drop([\"index\"], axis = 1)\n",
    "print(\"Combined: \" + str(dft.shape))\n",
    "#remove duplicated data instances\n",
    "dft = dft.drop_duplicates()\n",
    "dft = dft.reset_index()\n",
    "print(dft.shape)\n",
    "\n",
    "#extract information from data instance metadata\n",
    "jsonExtract = []\n",
    "for i in range(0, len(dft.content)):\n",
    "    jsonExtract.append(json.loads(str(dft.content[i])))\n",
    "names = list(jsonExtract[0])\n",
    "jsonDF = pd.DataFrame()\n",
    "for n in names:\n",
    "    nlist = []\n",
    "    for i in range(0, len(dft.content)):\n",
    "        if n in list(jsonExtract[i]):\n",
    "            nlist.append(jsonExtract[i][n])\n",
    "        else:\n",
    "            nlist.append(\"-100\")\n",
    "    jsonDF[n+\"2\"] = nlist\n",
    "dft = pd.concat([dft, jsonDF], axis = 1)#, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77309, 31)\n",
      "(36455, 31)\n",
      "Number of Messages: 36455\n",
      "Number of Participants: 134\n"
     ]
    }
   ],
   "source": [
    "#Limit data to ndays\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "indexes = []\n",
    "for i in range(0, dft.shape[0]):\n",
    "    timeEnd = datetime.fromtimestamp(dft.date[i]/1000)\n",
    "    timeEnd = timeEnd.replace(year = 2017)\n",
    "    timeStart = timeEnd - timedelta(days=ndays)\n",
    "    #timeCurrent = datetime.fromtimestamp(dft[\"date.1\"][i]/100)\n",
    "    timeCurrent = parser.parse(dft.created_at2[i])\n",
    "    timeCurrent = timeCurrent.replace(tzinfo = None)\n",
    "    diff = (timeStart-timeCurrent).days\n",
    "    if diff>0: #will be dropped\n",
    "        indexes.append(i)\n",
    "\n",
    "print(dft.shape)\n",
    "dft = dft.drop(indexes)\n",
    "print(dft.shape)\n",
    "\n",
    "print(\"Number of Messages: \" + str(dft.shape[0]))\n",
    "print(\"Number of Participants: \" + str(len(set(dft[\"id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet Participant by ID\n",
    "\n",
    "pDFt = pd.DataFrame()\n",
    "pID = []\n",
    "pContent = []\n",
    "nTweets = []\n",
    "score = []\n",
    "for i in set(dft[\"id\"]):\n",
    "    tempdf = dft[dft[\"id\"] == i].reset_index()\n",
    "    pID.append(i)\n",
    "    score.append(tempdf.scores[0])\n",
    "    p = []\n",
    "    for j in range(0, tempdf.shape[0]):\n",
    "        p.append(tempdf[\"full_text2\"][j])\n",
    "    pContent.append(p)\n",
    "    nTweets.append(len(p))\n",
    "pDFt[\"ID\"] = pID\n",
    "pDFt[\"Content\"] = pContent\n",
    "pDFt[\"Messages\"] = nTweets\n",
    "pDFt[\"Score\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "#tweet POS tags and sentiment\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "tags = []\n",
    "for i in range(0, len(pDFt.ID)):\n",
    "    print(i)\n",
    "    polarity2 = []\n",
    "    subjectivity2 = []\n",
    "    tags2 = []\n",
    "    for text in pDFt.Content[i]:\n",
    "        T = TextBlob(str(text))\n",
    "        polarity2.append(T.sentiment[0])\n",
    "        subjectivity2.append(T.sentiment[1])\n",
    "        for word, tag in T.tags:\n",
    "            tags2.append(tag)\n",
    "    tags.append(tags2)\n",
    "    polarity.append(polarity2)\n",
    "    subjectivity.append(subjectivity2)\n",
    "pDFt[\"POStags\"] = tags\n",
    "pDFt[\"Polarity\"] = polarity\n",
    "pDFt[\"Subjectivity\"] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#volume features for tweets\n",
    "words = []\n",
    "char = []\n",
    "for i in range(0, pDFt.shape[0]):\n",
    "    w = []\n",
    "    c = []\n",
    "    for tweet in pDFt.Content[i]:\n",
    "        w.append(len(tweet.split(\" \")))\n",
    "        c.append(len(tweet))\n",
    "    words.append(w)\n",
    "    char.append(c)\n",
    "pDFt[\"Words\"] = words\n",
    "pDFt[\"Characters\"] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real_estate', 'alcohol', 'anticipation', 'giving', 'violence', 'shopping', 'beauty', 'politeness', 'love', 'deception', 'musical', 'noise', 'kill', 'envy', 'valuable', 'vacation', 'breaking', 'hygiene', 'cold', 'anger', 'appearance', 'fear', 'nervousness', 'family', 'legend', 'fight', 'timidity', 'art', 'sadness', 'home', 'power', 'surprise', 'listen', 'technology', 'strength', 'computer', 'smell', 'fabric', 'anonymity', 'health', 'shame', 'leader', 'urban', 'college', 'play', 'cooking', 'swimming', 'plant', 'ship', 'phone', 'furniture', 'royalty', 'fun', 'monster', 'warmth', 'death', 'prison', 'worship', 'hipster', 'morning', 'poor', 'work', 'dispute', 'friends', 'water', 'neglect', 'hiking', 'disappointment', 'medieval', 'weapon', 'vehicle', 'help', 'ocean', 'independence', 'party', 'order', 'white_collar_job', 'sleep', 'messaging', 'traveling', 'ancient', 'sexual', 'youth', 'emotional', 'dance', 'school', 'torment', 'disgust', 'exercise', 'contentment', 'celebration', 'hate', 'pet', 'air_travel', 'superhero', 'ugliness', 'science', 'zest', 'heroic', 'dominant_personality', 'exasperation', 'weakness', 'horror', 'weather', 'affection', 'tourism', 'wealthy', 'night', 'meeting', 'children', 'body', 'tool', 'irritability', 'lust', 'animal', 'terrorism', 'magic', 'text_abbreviations', 'swearing_terms', 'fashion', 'suffering', 'childish', 'journalism', 'trust', 'medical_emergency', 'internet', 'speaking', 'fire', 'positive_emotion', 'music', 'negotiate', 'sympathy', 'rural', 'writing', 'healing', 'joy', 'communication', 'pain', 'sound', 'hearing', 'banking', 'military', 'money', 'masculine', 'reading', 'law', 'beach', 'social_media', 'restaurant', 'negative_emotion', 'driving', 'farming', 'stealing', 'philosophy', 'achievement', 'office', 'pride', 'car', 'leisure', 'shape_and_size', 'politics', 'exotic', 'attractive', 'domestic_work', 'gain', 'movement', 'blue_collar_job', 'dominant_heirarchical', 'economics', 'liquid', 'rage', 'competing', 'divine', 'cleaning', 'war', 'occupation', 'crime', 'optimism', 'business', 'wedding', 'sports', 'eating', 'ridicule', 'clothing', 'toy', 'feminine', 'programming', 'government', 'sailing', 'injury', 'payment', 'cheerfulness', 'confusion', 'aggression', 'religion']\n"
     ]
    }
   ],
   "source": [
    "from empath import Empath\n",
    "import re\n",
    "\n",
    "#create list of all empath categories\n",
    "\n",
    "lexicon = Empath()\n",
    "emp = lexicon.analyze(\"Testing\", normalize=True)\n",
    "wordlist = []\n",
    "for word, value in emp.items():\n",
    "    wordlist.append(word)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Empath features for Tweets\n",
    "\n",
    "for word in wordlist:\n",
    "    pctt = []\n",
    "    for i in range(0, pDFt.shape[0]):\n",
    "        content = re.sub(r'[^\\w\\s]', '', str(pDFt.Content[i]).lower())\n",
    "        lexicon = Empath()\n",
    "        emp = lexicon.analyze(content, categories=[word], normalize = True)\n",
    "        if emp != None:\n",
    "            for key, value in emp.items():\n",
    "                pctt.append(value)\n",
    "        else:\n",
    "            pctt.append(0)\n",
    "    pDFt[word] = pctt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ttyl\", \"brb\", \"brb\", \"lol\", \"lol\", \"welp\", \"lolol\", \"lol\", \"lolz\", \"lol\", \"lololol\", \"lol\", \"cuz\", \"wth\", \"lawl\", \"jk\", \"loool\", \"srsly\", \"lol\", \"Lol\", \"imma\", \"lmfao\", \"imma\", \"Jk\", \"lolololol\", \"lmao\", \"ikr\", \"cya\", \"imma\", \"Lolol\", \"nah\", \"hahaha\", \"lolol\", \"cus\", \"brb\", \"hahah\", \"ahaha\", \"aaand\", \"rn\", \"lool\", \"jk\", \"sorry_guys\", \"-_-\", \"mkay\", \"loll\", \"guyz\", \"fml\", \"Lol\", \"lol\", \"dw\", \"jk\", \"btw\", \"Lmao\", \"omg\", \"brb\", \"imma\", \"-__-\", \"ahahaha\", \"wtf\", \"omg\", \"lol\", \"nvm\", \"Bro\", \"srry\", \";p\", \"shiz\", \"hahahahaha\", \"aight\", \"naw\", \"ummm\", \"Ikr\", \"Brb\", \"lol\", \"sorry_bro\", \"Lololol\", \"ahha\", \"jk\", \"guyz\", \"aaaand\", \"smh\", \"bruh\", \"u\", \"Bruh\", \"hahaha\", \"ight\", \"cuz\", \"rofl\", \"Welp\", \"dw\", \":p\", \"cuz\", \"ur\", \"aaaaand\", \"haha\", \"yea\", \"gtg\", \"cuz\", \".now\", \"hey\", \"brb\"]\n"
     ]
    }
   ],
   "source": [
    "#create new category\n",
    "lexicon.create_category(\"text_abbreviations\",[\"lol\",\"ttyl\",\"brb\"], model=\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new catergory for tweets\n",
    "pctt = []\n",
    "empatht = []\n",
    "for i in range(0, pDFt.shape[0]):\n",
    "    content = re.sub(r'[^\\w\\s]', '', str(pDFt.Content[i]).lower())\n",
    "    empatht.append(len(content.split(\" \")))\n",
    "    lexicon = Empath()\n",
    "    emp = lexicon.analyze(content, categories=[\"text_abbreviations\"], normalize = True)\n",
    "    if emp != None:\n",
    "        for key, value in emp.items():\n",
    "            pctt.append(value)\n",
    "    else:\n",
    "        pctt.append(0)\n",
    "pDFt[\"text_abbreviations\"] = pctt\n",
    "pDFt[\"WordsEmpath\"] = empatht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WP$', 'JJ', 'NNS', 'TO', 'JJS', 'RBR', 'LS', 'NN', 'PRP', 'IN', 'UH', 'CD', 'PDT', 'VBG', 'VBD', 'MD', 'SYM', 'NNP', 'JJR', 'POS', 'WRB', 'VB', 'VBP', 'FW', 'CC', 'VBZ', 'PRP$', 'RP', 'WP', 'RBS', 'WDT', 'DT', 'EX', 'VBN', 'RB', 'NNPS'}\n"
     ]
    }
   ],
   "source": [
    "#get set of POS tags\n",
    "posTags = []\n",
    "for i in range(0, pDFt.shape[0]):\n",
    "    for tag in pDFt.POStags[i]:\n",
    "        posTags.append(tag)\n",
    "posSet = set(posTags)\n",
    "print(posSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS tag counting for tweets\n",
    "\n",
    "poswordst = []\n",
    "for posList in pDFt.POStags:\n",
    "    poswordst.append(len(posList))\n",
    "pDFt[\"WordsTags\"] = poswordst\n",
    "\n",
    "for tag in posSet:\n",
    "    cnt = []\n",
    "    for posList in pDFt.POStags:\n",
    "        counter = 0\n",
    "        for item in posList:\n",
    "            if item == tag:\n",
    "                counter += 1\n",
    "        cnt.append(counter)\n",
    "    pDFt[tag] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentiment features for tweets\n",
    "\n",
    "pcount = []\n",
    "ncount = []\n",
    "pstd = []\n",
    "nstd = []\n",
    "pavg = []\n",
    "navg = []\n",
    "scount = []\n",
    "sstd = []\n",
    "savg = []\n",
    "for i in range(0, pDFt.shape[0]):\n",
    "    s = []\n",
    "    p = []\n",
    "    n = []\n",
    "    for item in pDFt.Polarity[i]:\n",
    "        if item > 0:\n",
    "            p.append(item)\n",
    "        if item < 0:\n",
    "            n.append(item)\n",
    "    for item in pDFt.Subjectivity[i]:\n",
    "        if item > 0:\n",
    "            s.append(item)\n",
    "    pcount.append(len(p))\n",
    "    ncount.append(len(n))\n",
    "    scount.append(len(s))\n",
    "    if len(p) > 0:\n",
    "        pavg.append(sum(p)/len(p))\n",
    "        pstd.append(np.std(p))\n",
    "    else:\n",
    "        pavg.append(0)\n",
    "        pstd.append(0)\n",
    "    if len(n) > 0:\n",
    "        navg.append(sum(n)/len(n))\n",
    "        nstd.append(np.std(n))\n",
    "    else:\n",
    "        navg.append(0)\n",
    "        nstd.append(0)\n",
    "    if len(s) > 0:\n",
    "        savg.append(sum(s)/len(s))\n",
    "        sstd.append(np.std(s))\n",
    "    else:\n",
    "        savg.append(0)\n",
    "        sstd.append(0)\n",
    "pDFt[\"PositiveCnt\"] = pcount\n",
    "pDFt[\"NegativeCnt\"] = ncount\n",
    "pDFt[\"PositiveStd\"] = pstd\n",
    "pDFt[\"NegativeStd\"] = nstd\n",
    "pDFt[\"PostitiveAvg\"] = pavg\n",
    "pDFt[\"NegativeAvg\"] = navg\n",
    "pDFt[\"SubjectiveCnt\"] = scount\n",
    "pDFt[\"SubjectiveStd\"] = sstd\n",
    "pDFt[\"SubjectiveAvg\"] = savg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Volume features for Tweets\n",
    "wsum = []\n",
    "wavg = []\n",
    "wstd = []\n",
    "csum = []\n",
    "cavg = []\n",
    "cstd = []\n",
    "unique = []\n",
    "\n",
    "for i in range(0, pDFt.shape[0]):\n",
    "    wsum.append(sum(pDFt.Words[i]))\n",
    "    wavg.append(sum(pDFt.Words[i])/len(pDFt.Words[i]))\n",
    "    wstd.append(np.std(pDFt.Words[i]))\n",
    "    csum.append(sum(pDFt.Characters[i]))\n",
    "    cavg.append(sum(pDFt.Characters[i])/len(pDFt.Characters[i]))\n",
    "    cstd.append(np.std(pDFt.Characters[i]))\n",
    "    unique.append(len(list(set(pDFt.Content[i]))))\n",
    "\n",
    "pDFt[\"WordSum\"] = wsum\n",
    "pDFt[\"WordAvg\"] = wavg\n",
    "pDFt[\"WordStd\"] = wstd\n",
    "pDFt[\"CharacterSum\"] = csum\n",
    "pDFt[\"CharacterAvg\"] = cavg\n",
    "pDFt[\"CharacterStd\"] = cstd\n",
    "pDFt[\"UniqueCnt\"] = unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveDFtv = pDFt.drop(columns = [\"Content\", \"POStags\", \"Polarity\", \"Subjectivity\", \"Words\", \"Characters\"])\n",
    "saveDFtv.to_csv(\"preprocessed\" + modality + str(ndays) + \"days.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
